{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('./notebooks/')\n",
    "from _load_results import *\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statannotations.Annotator import Annotator #https://github.com/trevismd/statannotations\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "def load_results():\n",
    "    results = []\n",
    "    for results_path in [results_path_llms, results_path_not_llms]:\n",
    "        for file in glob(\n",
    "            f\"{results_path}/**/*.json\",\n",
    "            recursive=True,\n",
    "        ):\n",
    "            df = pd.read_json(file, typ=\"series\")\n",
    "    \n",
    "            indxs = df.index\n",
    "            # indxs = [indx for indx in indxs if indx not in ['pred', 'pred_proba']]\n",
    "    \n",
    "            results.append(df[indxs])\n",
    "    results_df = pd.DataFrame(data=results, columns=indxs)\n",
    "\n",
    "    # Beautifying it\n",
    "    results_df[\"model\"] = results_df[\"model\"].apply(lambda m: nice_model_labels[m])\n",
    "    results_df[\"target\"] = results_df[\"target\"].apply(lambda t: dnames_to_nice[t])\n",
    "    \n",
    "    results_df = results_df[results_df[\"model\"].isin(order)]\n",
    "    \n",
    "    print(results_df.shape)\n",
    "    print(results_df[\"model\"].unique())\n",
    "    print(results_df[\"target\"].unique())\n",
    "    return results_df\n",
    "\n",
    "results_df = load_results()\n",
    "\n",
    "%matplotlib inline\n",
    "paper_dir = './paper/floats/'\n",
    "boxplot_kwargs = {\n",
    "    # 'sharey':True,\n",
    "    'notch':False,\n",
    "    'showcaps':True,\n",
    "    'flierprops':{\"marker\": \"x\"},\n",
    "    'boxprops':{\"facecolor\": 'white'},\n",
    "    'medianprops':{\"color\": \"r\", \"linewidth\": 2}\n",
    "}\n",
    "\n",
    "phenotypes_order =  ['HTN Heuristic', 'Htn-Hypokalemia Heuristic', 'Resistant HTN Heuristic',\n",
    "                     'HTN Diagnosis', 'HTN-Hypokalemia Diagnosis', 'Resistant HTN Diagnosis']\n",
    "\n",
    "# Making it the format seaborn likes\n",
    "results_df_melted = pd.melt(\n",
    "    results_df, \n",
    "    id_vars=['model', 'target', 'fold', 'RunID', 'random_state', 'prompt_richness', 'icd_only']\n",
    ")\n",
    "\n",
    "print(results_df_melted['random_state'].unique())\n",
    "# display(results_df_melted.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of results\n",
    "\n",
    "# group by different experimental settings, count the occurence of experiments (any of\n",
    "# 'fold', 'RunID', 'random_state' should do it), then pivot to fit everything in the screen\n",
    "\n",
    "# 'prompt_richness', 'icd_only' --> these should be irrelevant, as I am loading a single setting\n",
    "# (the best one from the 00 notebook analysis) for the LLMs.\n",
    "\n",
    "results_df \\\n",
    "    .groupby(['model', 'target', ]) \\\n",
    "    .count()[['fold', 'RunID', 'random_state']] \\\n",
    "    .pivot_table(index=['target'], columns=['model'],values='random_state') \\\n",
    "    .fillna(0).astype('int').style.background_gradient(axis=None, cmap='viridis')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_melted.loc[results_df_melted['model'].isin(['DT','FEAT','LR L1','RF'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying Fig.2 from paper \"A flexible symbolic regression method for constructing\n",
    "# interpretable clinical prediction models\"\n",
    "palette = sns.color_palette(\"hls\", 6) # ['#374aa3', '#cc6666', '#6688d0', '#ffcccc', '#336699', '#99ccff']\n",
    "sns.set_theme(\n",
    "    context='paper',\n",
    "    style='whitegrid',\n",
    "    font_scale=1.5\n",
    ")\n",
    "prompt_richness=True\n",
    "icd_only=True\n",
    "stat_comparisons=True\n",
    "for prompt_richness in [True,False]:\n",
    "    for icd_only in [True,False]:\n",
    "        print(\"prompt_richness\",prompt_richness,\"expert_features\",icd_only)\n",
    "# for stat_comparisons in [True]:\n",
    "        for metric in [\n",
    "            'average_precision_score_test', \n",
    "            # 'roc_auc_score_test', \n",
    "            # 'accuracy_score_test', \n",
    "            # 'size'\n",
    "        ]:\n",
    "            data = (\n",
    "                results_df_melted[\n",
    "                    (results_df_melted['variable']==metric)\n",
    "                    & (\n",
    "                        (results_df_melted['model'].isin(['DT','FEAT','LR L1','RF']))\n",
    "                        | \n",
    "                        (\n",
    "                            (results_df_melted['prompt_richness']==prompt_richness)\n",
    "                            & (results_df_melted['icd_only']==icd_only)\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "                .dropna()\n",
    "            )\n",
    "                       \n",
    "            metric = metric.replace('average_precision_score_test', 'AUPRC') \n",
    "    \n",
    "            data = data.rename(columns={'value': metric}) #, 'model':'Model'})\n",
    "            data[metric] = data[metric].astype(float)\n",
    "            data['Model'] = data['model'].apply(lambda x: x.replace('-iter',''))\n",
    "            data['Strategy'] = ['SEDI / Train' if 'iter' in v or 'gpt' not in v else 'Zero Shot' for v in data['model'].values]\n",
    "            hue_order = ['Zero Shot', 'SEDI / Train']\n",
    "            \n",
    "            # print(data.random_state.unique())\n",
    "            # display(data)\n",
    "    \n",
    "            new_order=[o for o in order if o in data['Model'].unique()]\n",
    "            g = sns.catplot(\n",
    "                data=data,\n",
    "                x=\"Model\", \n",
    "                order=new_order, \n",
    "                y=metric, \n",
    "                col=\"target\",\n",
    "                col_wrap=3, \n",
    "                col_order=phenotypes_order,\n",
    "                aspect= 1 if stat_comparisons else 0.825,\n",
    "                height= 4.5 if stat_comparisons else 3.25,\n",
    "                estimator=np.median, \n",
    "                sharey=not stat_comparisons,\n",
    "                hue='Strategy',\n",
    "                hue_order=hue_order,\n",
    "                # kind=\"box\", \n",
    "                # **boxplot_kwargs\n",
    "                kind='bar',\n",
    "                # dodge=True, \n",
    "                capsize=.4,\n",
    "                err_kws={\"color\": \".5\", \"linewidth\": 1.5},\n",
    "                edgecolor=\".5\",\n",
    "                palette=palette\n",
    "            )\n",
    "            # g.set_ylabels(metric.replace('_', ' ').replace(' test','').title())\n",
    "            g.set_ylabels(metric.replace('_', ' ').replace(' test',''))\n",
    "            # g.refline(x=2.5, color='k', lw=0.5, ls='--', zorder=0)\n",
    "            \n",
    "            for (title, ax) in g._axes_dict.items():\n",
    "                ax.plot([2.5,2.5], [0, 1.0], '--k', zorder=0, lw=0.5) #color='k', lw=0.5, ls='--', zorder=0)\n",
    "                # ax.set_title(title)\n",
    "                ax.set_title(\n",
    "                    title\n",
    "                    .replace('Diagnosis','Dx')\n",
    "                    .replace('Resistant HTN', 'aTRH')\n",
    "                    .replace('Hypokalemia','HypoK')\n",
    "                )\n",
    "                ax.grid(which='major', axis='y', linewidth=.8)\n",
    "                ax.grid(which='major', axis='x', linewidth=.5)\n",
    "                \n",
    "                for tick in ax.get_xticklabels():            \n",
    "                    if stat_comparisons:\n",
    "                        tick.set(rotation=45, ha='right', va='top', ma='center')\n",
    "                    else:\n",
    "                        tick.set(rotation=90, ha='center', va='top', ma='center')\n",
    "                # yticks,yticklabels =  zip(*[\n",
    "                #          (yt,yl) for yt,yl in zip(ax.get_yticks(),ax.get_yticklabels()) if yl not in ['1.1','1.2']\n",
    "                # ])\n",
    "                # ax.set_yticks(yticks, yticklabels)\n",
    "                # ax.set_yticks(yticks)\n",
    "                if metric=='Size':\n",
    "                    ax.set_yscale('log')\n",
    "                elif 'roc' in metric.lower():\n",
    "                    ax.set_ylim(bottom=0.5)\n",
    "                # Loop over the bars\n",
    "                for i,(thisbar, thisline) in enumerate(zip(ax.patches,ax.lines)):\n",
    "                    # Set a different hatch for each bar\n",
    "                    # thisbar.set_hatch(hatches[i])\n",
    "                    if i > 5:\n",
    "                        w = thisbar.get_width()\n",
    "                        thisbar.set_x(thisbar.get_x()-w)\n",
    "                        thisbar.set_width(w*2)\n",
    "                        thisline.set_xdata([x-w/2 for x in thisline.get_xdata()])\n",
    "                        # import pdb\n",
    "                        # pdb.set_trace()\n",
    "    \n",
    "                # All vs baseline\n",
    "                llms = [m for m in data['Model'].unique() if 'gpt' in m]\n",
    "                # llms = [\n",
    "                #     'gpt-4o-mini',\n",
    "                #     'gpt-4o-mini-iter',\n",
    "                #     'gpt-4o',\n",
    "                #     'gpt-4o-iter',\n",
    "                #     'gpt-4-turbo',\n",
    "                #     'gpt-4-turbo-iter',\n",
    "                # ]\n",
    "                pairs = (\n",
    "                    # list(itertools.product(llms, llms)) +\n",
    "                    [((llm,'SEDI / Train'), ('FEAT','SEDI / Train')) for llm in llms ]\n",
    "                    + \n",
    "                    [((llm,'SEDI / Train'), (llm,'Zero Shot')) for llm in llms ]\n",
    "                )\n",
    "    \n",
    "                # ax.set_ylim(top=1.4)\n",
    "                if stat_comparisons:\n",
    "                    try:\n",
    "                        annotator = Annotator(\n",
    "                            ax,\n",
    "                            pairs, \n",
    "                            data=data[(data['target']==title)], #[ ['Model', 'target',\tmetric, 'Strategy'] ],\n",
    "                            x='Model', y=metric,\n",
    "                            order=new_order,\n",
    "                            hue='Strategy',\n",
    "                            hue_order=hue_order,\n",
    "                        )\n",
    "                        annotator.reset_configuration()\n",
    "                        annotator.configure(\n",
    "                            test=\"Mann-Whitney\", \n",
    "                            hide_non_significant=True, \n",
    "                            # text_format='simple',\n",
    "                            text_format='star',\n",
    "                            comparisons_correction=\"holm\", \n",
    "                            verbose=1\n",
    "                        ) # , text_offset=-3\n",
    "                        \n",
    "                        # Calculate and annotate\n",
    "                        annotator.apply_and_annotate()\n",
    "                    except Exception as e:\n",
    "                        print(f\"failed to add statsannotation with dataset {title}. Except {e}\")\n",
    "    \n",
    "                    yticklabels = ax.get_yticklabels()\n",
    "                    yticklabels = [yl if float(yl._text) < 1.001 else '' for yl in yticklabels]\n",
    "                    ax.set_yticks(ax.get_yticks(),labels=yticklabels)\n",
    "    \n",
    "                    \n",
    "            # g.map_dataframe(sns.stripplot, y=metric, dodge=True, \n",
    "            #                 x=\"Model\", \n",
    "            #                 order=new_order, \n",
    "            #                 hue='Strategy', \n",
    "            #                 hue_order=['Zero Shot','Learning'],\n",
    "                            # hue_order=['Zero Shot','Learning'],\n",
    "            #                 size=2, color='k',\n",
    "            #                 linewidth=0.5, \n",
    "            #                 alpha=0.25)\n",
    "            # plt.tight_layout()\n",
    "            sns.move_legend(g,loc=[.2,.65] if stat_comparisons else [0.2,0.7],frameon=True, framealpha=1)\n",
    "            figname = (\n",
    "                f\"{paper_dir}/bar_{metric}{'_stat_comparisons' if stat_comparisons else ''}_{prompt_richness}_{icd_only}.pdf\"\n",
    "            )\n",
    "            print('saving',figname)\n",
    "            \n",
    "            plt.savefig(figname, bbox_inches='tight' )\n",
    "            # plt.savefig(f\"{paper_dir}/bar_{metric}{'_stat_comparisons' if stat_comparisons else ''}.png\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis_metrics = ['average_precision_score_test', 'roc_auc_score_test', 'accuracy_score_test']\n",
    "phenotypes = ['Diagnosis', 'Heuristic']\n",
    "\n",
    "for target in results_df_melted['target'].unique():\n",
    "    print(target)\n",
    "\n",
    "    data = results_df_melted[ \n",
    "        ( results_df_melted['variable'].isin(yaxis_metrics) )\n",
    "        & ( results_df_melted['target'].str.contains(target) ) ]\n",
    "\n",
    "    # Aggregate data\n",
    "    aggregated_data = data.groupby(['model', 'variable'])['value'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    # Pivot the data\n",
    "    pivoted_data = aggregated_data.pivot(index='model', columns=['variable'], values=['mean', 'std']).fillna(0)\n",
    "    \n",
    "    # Flatten column names\n",
    "    pivoted_data.columns = [f\"{col[0]}_{col[1]}\" for col in pivoted_data.columns]\n",
    "    \n",
    "    # display(pivoted_data)\n",
    "    \n",
    "    # Create a new DataFrame with mean ± std format\n",
    "    formatted_data = pd.DataFrame({\n",
    "        yaxis: pivoted_data[f\"mean_{yaxis}\"].astype(str) + r'\\pm' + pivoted_data[f\"std_{yaxis}\"].astype(str)\n",
    "        for yaxis in yaxis_metrics\n",
    "    }, index=pivoted_data.index)\n",
    "    \n",
    "    # Format the values to two decimal places\n",
    "    formatted_data = formatted_data.applymap(lambda x: f\"${float(x.split(r'\\pm')[0]):.2f} \\pm {float(x.split(r'\\pm')[1]):.2f}$\")\n",
    "    \n",
    "    display(formatted_data)\n",
    "    \n",
    "    # Create LaTeX table\n",
    "    latex_table = formatted_data.to_latex(\n",
    "        index=True,\n",
    "        # caption=f\"{yaxis.capitalize().replace('_', ' ')} scores for {target} models\",\n",
    "        # label=f\"tab:{target.lower()}_{yaxis.replace('_', '')}\",\n",
    "        position=\"htbp\",\n",
    "        column_format=\"lrrr\",\n",
    "        escape=False\n",
    "    )\n",
    "    \n",
    "    # Save LaTeX table to file\n",
    "    filename = f\"{paper_dir}/tab_{target.lower().replace(' ','-')}.tex\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    \n",
    "    print(f\"\\nLaTeX table saved to {filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis = 'roc_auc_score_test' # 'average_precision_score_test' 'accuracy_score_test'\n",
    "\n",
    "for phenotype in ['Heuristic', 'Diagnosis']:\n",
    "    print(phenotype)\n",
    "\n",
    "    data = results_df_melted[ \n",
    "        ( results_df_melted['variable']==yaxis )\n",
    "        & ( results_df_melted['target'].str.contains(phenotype) ) ]\n",
    "\n",
    "    # Aggregate data\n",
    "    aggregated_data = data.groupby(['model', 'target'])['value'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    # Pivot the data\n",
    "    pivoted_data = aggregated_data.pivot(index='model', columns=['target'], values=['mean', 'std']).fillna(0)\n",
    "    \n",
    "    # Flatten column names\n",
    "    pivoted_data.columns = [f\"{col[0]}_{col[1]}\" for col in pivoted_data.columns]\n",
    "    \n",
    "    # display(pivoted_data)\n",
    "    # print(data['target'].unique())\n",
    "    \n",
    "    # Create a new DataFrame with mean ± std format\n",
    "    formatted_data = pd.DataFrame({\n",
    "        t: pivoted_data[f\"mean_{t}\"].astype(str) + r'\\pm' + pivoted_data[f\"std_{t}\"].astype(str)\n",
    "        for t in [p for p in phenotypes_order if p in data['target'].unique()]\n",
    "    }, index=pivoted_data.index)\n",
    "\n",
    "    # Format the values to two decimal places\n",
    "    formatted_data = formatted_data.applymap(lambda x: f\"${float(x.split(r'\\pm')[0]):.2f} \\pm {float(x.split(r'\\pm')[1]):.2f}$\")\n",
    "    \n",
    "    display(formatted_data)\n",
    "    \n",
    "    # Create LaTeX table\n",
    "    latex_table = formatted_data.to_latex(\n",
    "        index=True,\n",
    "        caption=f\"{yaxis.capitalize().replace('_', ' ')} scores for {phenotype} targets\",\n",
    "        label=f\"tab:{target.lower()}_{yaxis.replace('_', '')}\",\n",
    "        position=\"htbp\",\n",
    "        column_format=\"lrrr\",\n",
    "        escape=False\n",
    "    )\n",
    "    \n",
    "    # Save LaTeX table to file\n",
    "    filename = f\"{paper_dir}/tab_{phenotype}.tex\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    \n",
    "    print(f\"\\nLaTeX table saved to {filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying Fig.3 from paper \"A flexible symbolic regression method for constructing\n",
    "# interpretable clinical prediction models\"\n",
    "\n",
    "palette = sns.color_palette(\"hls\", 7) # ['#374aa3', '#cc6666', '#6688d0', '#ffcccc', '#336699', '#99ccff']\n",
    "sns.set_theme(\n",
    "    context='paper',\n",
    "    style='whitegrid',\n",
    "    font_scale=1.5\n",
    ")\n",
    "\n",
    "for yaxis in ['average_precision_score_test', 'roc_auc_score_test']:\n",
    "    data = (\n",
    "        results_df[\n",
    "            (results_df['model'].isin(['DT','FEAT','LR L1','RF']))\n",
    "            | \n",
    "            (\n",
    "                (results_df['prompt_richness']==True)\n",
    "                & ( (results_df['icd_only']==True)\n",
    "                  & (results_df['model'].str.contains('iter')))\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(data.model.unique())\n",
    "    data['Strategy'] = ['SEDI' if 'iter' in v else 'Zero Shot' for v in data['model'].values]\n",
    "    #data = data[data['Strategy']=='SEDI']\n",
    "\n",
    "    data = data.rename(columns={yaxis: yaxis.replace('_', ' ').capitalize(), 'model':'Model', 'size':'Size'})\n",
    "    yaxis = yaxis.replace('_', ' ').capitalize()\n",
    "    \n",
    "    data['Model'] = data['Model'].apply(lambda x: x.replace('-iter',''))\n",
    "\n",
    "    def lower_ci(x): return x.quantile(0.05)\n",
    "    def upper_ci(x): return x.quantile(0.95)\n",
    "\n",
    "    data = data.groupby(['target', 'Model'])[[yaxis, 'Size']].agg(['mean', 'std', lower_ci, upper_ci]).reset_index()\n",
    "\n",
    "    model_order = [o for o in order if o in data.Model.unique()]\n",
    "    data.columns = list(map(''.join, data.columns.values))\n",
    "\n",
    "    print(data.columns)\n",
    "\n",
    "    phenotypes_order =  ['HTN-Hypokalemia Diagnosis']\n",
    "\n",
    "    g = sns.relplot(\n",
    "        data=data,\n",
    "        x=\"Sizemean\", y=f\"{yaxis}mean\", aspect=1, height=5, \n",
    "        col=\"target\", col_wrap=1, col_order=phenotypes_order,\n",
    "        kind=\"scatter\",\n",
    "        hue='Model', # hue_order=model_order,\n",
    "        style ='Model', # style_order=model_order,\n",
    "        linewidth=1.0, s=150, alpha=1.0, # sharey=False\n",
    "        edgecolor='k', \n",
    "        facet_kws={'sharey': False, 'sharex': False}\n",
    "    )\n",
    "\n",
    "    for (title, plot_ax) in g._axes_dict.items():\n",
    "        plot_ax.set_title(\n",
    "            title\n",
    "            .replace('Diagnosis','Dx')\n",
    "            .replace('Resistant HTN', 'aTRH')\n",
    "            .replace('Hypokalemia','HypoK')\n",
    "        )\n",
    "        plot_ax.grid(which='major', axis='y', linewidth=.8, ls=':', zorder=-9999)\n",
    "        plot_ax.grid(which='major', axis='x', linewidth=.5, ls=':', zorder=-9999)\n",
    "    \n",
    "        plot_ax.set_xlabel(plot_ax.get_xlabel().replace('mean', '') if plot_ax.get_xlabel() else '')\n",
    "        plot_ax.set_ylabel(plot_ax.get_ylabel().replace('mean', '') if plot_ax.get_ylabel() else '')\n",
    "        plot_ax.set_ylabel(plot_ax.get_ylabel().replace('Average precision score test', 'AUPRC') if plot_ax.get_ylabel() else '')\n",
    "\n",
    "        # for tick in plot_ax.get_xticklabels():\n",
    "        #     tick.set(rotation=30, ha='center', va='top', ma='right')\n",
    "\n",
    "        mask = data['target'] == title\n",
    "        x_data = data[mask]['Sizemean']\n",
    "        x_std = data[mask]['Sizestd']\n",
    "        x_lower = data[mask][\"Sizelower_ci\"]\n",
    "        x_upper = data[mask][\"Sizeupper_ci\"]\n",
    "\n",
    "        y_data = data[mask][f\"{yaxis}mean\"]\n",
    "        y_std = data[mask][f\"{yaxis}std\"]\n",
    "        y_lower = data[mask][f\"{yaxis}lower_ci\"]\n",
    "        y_upper = data[mask][f\"{yaxis}upper_ci\"]\n",
    "        \n",
    "        for i, (x, x_std, x_l, x_u, y, y_std, y_l, y_u) in enumerate(zip(\n",
    "            x_data, x_std, x_lower, x_upper, y_data, y_std, y_lower, y_upper)):\n",
    "\n",
    "            plot_ax.errorbar(\n",
    "                x=x, \n",
    "                y=y, \n",
    "                # xerr=x_std,\n",
    "                # yerr=y_std,\n",
    "                xerr=[[abs(x - x_l)], [abs(x - x_u)]],\n",
    "                yerr=[[abs(y - y_l)], [abs(y - y_u)]],\n",
    "                fmt='none',  # No marker, just error bars\n",
    "                color='k', #, palette[i],\n",
    "                capsize=2,\n",
    "                elinewidth=1.25,\n",
    "                capthick=1.25,\n",
    "                alpha=1.0,\n",
    "                zorder=-999\n",
    "            )\n",
    "\n",
    "    g.set(xscale=\"log\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.move_legend(g, \"upper left\", bbox_to_anchor=(0.4, 0.55), ncol=2, frameon=True)\n",
    "    plt.savefig(f\"{paper_dir}/pareto_grouped_aTRH_{yaxis}.pdf\")\n",
    "    plt.savefig(f\"{paper_dir}/pareto_grouped_aTRH_{yaxis}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying Fig.3 from paper \"A flexible symbolic regression method for constructing\n",
    "# interpretable clinical prediction models\"\n",
    "\n",
    "palette = sns.color_palette(\"hls\", 7) # ['#374aa3', '#cc6666', '#6688d0', '#ffcccc', '#336699', '#99ccff']\n",
    "sns.set_theme(\n",
    "    context='paper',\n",
    "    style='whitegrid',\n",
    "    font_scale=1.5\n",
    ")\n",
    "\n",
    "for yaxis in ['average_precision_score_test', 'roc_auc_score_test']:\n",
    "    data = (\n",
    "        results_df[\n",
    "            (results_df['model'].isin(['DT','FEAT','LR L1','RF']))\n",
    "            | \n",
    "            (\n",
    "                (results_df['prompt_richness']==True)\n",
    "                & ( (results_df['icd_only']==True)\n",
    "                  & (results_df['model'].str.contains('iter')))\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(data.model.unique())\n",
    "    data['Strategy'] = ['SEDI' if 'iter' in v else 'Zero Shot' for v in data['model'].values]\n",
    "    #data = data[data['Strategy']=='SEDI']\n",
    "\n",
    "    data = data.rename(columns={yaxis: yaxis.replace('_', ' ').capitalize(), 'model':'Model', 'size':'Size'})\n",
    "    yaxis = yaxis.replace('_', ' ').capitalize()\n",
    "    \n",
    "    data['Model'] = data['Model'].apply(lambda x: x.replace('-iter',''))\n",
    "\n",
    "    def lower_ci(x): return x.quantile(0.05)\n",
    "    def upper_ci(x): return x.quantile(0.95)\n",
    "\n",
    "    data = data.groupby(['target', 'Model'])[[yaxis, 'Size']].agg(['mean', 'std', lower_ci, upper_ci]).reset_index()\n",
    "\n",
    "    model_order = [o for o in order if o in data.Model.unique()]\n",
    "    data.columns = list(map(''.join, data.columns.values))\n",
    "\n",
    "    print(data.columns)\n",
    "\n",
    "    # phenotypes_order =  ['Resistant HTN Heuristic', 'HTN Heuristic', 'Htn-Hypokalemia Heuristic', \n",
    "    #                      'Resistant HTN Diagnosis', 'HTN Diagnosis', 'HTN-Hypokalemia Diagnosis']\n",
    "\n",
    "    phenotypes_order =  ['HTN Heuristic', 'Htn-Hypokalemia Heuristic', 'Resistant HTN Heuristic',\n",
    "                         'HTN Diagnosis', 'HTN-Hypokalemia Diagnosis', 'Resistant HTN Diagnosis']\n",
    "\n",
    "    g = sns.relplot(\n",
    "        data=data,\n",
    "        x=\"Sizemean\", y=f\"{yaxis}mean\", aspect=1, height=3.5, \n",
    "        col=\"target\", col_wrap=3, col_order=phenotypes_order,\n",
    "        kind=\"scatter\",\n",
    "        hue='Model', # hue_order=model_order,\n",
    "        style ='Model', # style_order=model_order,\n",
    "        linewidth=1.0, s=150, alpha=1.0, # sharey=False\n",
    "        edgecolor='k', \n",
    "        facet_kws={'sharey': False, 'sharex': False}\n",
    "    )\n",
    "\n",
    "    for (title, plot_ax) in g._axes_dict.items():\n",
    "        plot_ax.set_title(\n",
    "            title\n",
    "            .replace('Diagnosis','Dx')\n",
    "            .replace('Resistant HTN', 'aTRH')\n",
    "            .replace('Hypokalemia','HypoK')\n",
    "        )\n",
    "        plot_ax.grid(which='major', axis='y', linewidth=.8, ls=':', zorder=-9999)\n",
    "        plot_ax.grid(which='major', axis='x', linewidth=.5, ls=':', zorder=-9999)\n",
    "    \n",
    "        plot_ax.set_xlabel(plot_ax.get_xlabel().replace('mean', '') if plot_ax.get_xlabel() else '')\n",
    "        plot_ax.set_ylabel(plot_ax.get_ylabel().replace('mean', '') if plot_ax.get_ylabel() else '')\n",
    "        plot_ax.set_ylabel(plot_ax.get_ylabel().replace('Average precision score test', 'AUPRC') if plot_ax.get_ylabel() else '')\n",
    "\n",
    "        # for tick in plot_ax.get_xticklabels():\n",
    "        #     tick.set(rotation=30, ha='center', va='top', ma='right')\n",
    "\n",
    "        mask = data['target'] == title\n",
    "        x_data = data[mask]['Sizemean']\n",
    "        x_std = data[mask]['Sizestd']\n",
    "        x_lower = data[mask][\"Sizelower_ci\"]\n",
    "        x_upper = data[mask][\"Sizeupper_ci\"]\n",
    "\n",
    "        y_data = data[mask][f\"{yaxis}mean\"]\n",
    "        y_std = data[mask][f\"{yaxis}std\"]\n",
    "        y_lower = data[mask][f\"{yaxis}lower_ci\"]\n",
    "        y_upper = data[mask][f\"{yaxis}upper_ci\"]\n",
    "        \n",
    "        for i, (x, x_std, x_l, x_u, y, y_std, y_l, y_u) in enumerate(zip(\n",
    "            x_data, x_std, x_lower, x_upper, y_data, y_std, y_lower, y_upper)):\n",
    "\n",
    "            plot_ax.errorbar(\n",
    "                x=x, \n",
    "                y=y, \n",
    "                # xerr=x_std,\n",
    "                # yerr=y_std,\n",
    "                xerr=[[abs(x - x_l)], [abs(x - x_u)]],\n",
    "                yerr=[[abs(y - y_l)], [abs(y - y_u)]],\n",
    "                fmt='none',  # No marker, just error bars\n",
    "                color='k', #, palette[i],\n",
    "                capsize=2,\n",
    "                elinewidth=1.25,\n",
    "                capthick=1.25,\n",
    "                alpha=1.0,\n",
    "                zorder=-999\n",
    "            )\n",
    "\n",
    "    g.set(xscale=\"log\")\n",
    "\n",
    "\n",
    "    sns.move_legend(g, \"upper left\", bbox_to_anchor=(0.8, 0.75), ncol=2, fontsize=9, frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{paper_dir}/pareto_grouped_{yaxis}.pdf\")\n",
    "    plt.savefig(f\"{paper_dir}/pareto_grouped_{yaxis}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying Fig.3 from paper \"A flexible symbolic regression method for constructing\n",
    "# interpretable clinical prediction models\"\n",
    "for yaxis in ['average_precision_score_test', 'roc_auc_score_test']:\n",
    "    g = sns.relplot(\n",
    "        data=results_df,\n",
    "        x=\"size\", y=yaxis, col=\"target\", \n",
    "        col_order = dnames_nice,\n",
    "        kind=\"scatter\",\n",
    "        style='model', markers=marker_choice,\n",
    "        hue = \"model\", hue_order=order, col_wrap=3,\n",
    "        linewidth=0.5, s=100\n",
    "    )\n",
    "    for (ds, plot_ax) in g._axes_dict.items():\n",
    "        plot_ax.grid(which='major', axis='y', linewidth=.5)\n",
    "        plot_ax.grid(which='major', axis='x', linewidth=.5)\n",
    "        \n",
    "        for tick in plot_ax.get_xticklabels():\n",
    "            tick.set(rotation=30, ha='center', va='top', ma='right')\n",
    "\n",
    "    g.set(xscale=\"log\")\n",
    "\n",
    "    plt.savefig(f\"{paper_dir}/fig3_{yaxis}.pdf\")\n",
    "    plt.savefig(f\"{paper_dir}/fig3_{yaxis}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(pred):\n",
    "    pred = pred.replace('\\n','')\n",
    "    pred = pred.replace('[','')\n",
    "    pred = pred.replace(']','')\n",
    "    pred = list(map(float,pred.split()))\n",
    "    return pred\n",
    "\n",
    "def prc_values(y,y_pred_proba):\n",
    "    precision, recall, prcthresholds = precision_recall_curve(y, y_pred_proba, pos_label=1)\n",
    "    precision[-1] = np.max(precision[:-1])\n",
    "    s = np.argsort(recall)\n",
    "    precision = precision[s]\n",
    "    recall = recall[s]\n",
    "    mean_recall = np.linspace(0.0, 1, 21)\n",
    "    precision = interp(mean_recall, recall, precision)\n",
    "    return mean_recall, precision\n",
    "\n",
    "def roc_values(y,y_pred_proba):\n",
    "    fpr,tpr, rocthresholds = roc_curve(y, y_pred_proba, pos_label=1)\n",
    "    roc = pd.DataFrame(list(zip(fpr,tpr, rocthresholds)), columns =['fpr','tpr','thresholds']) \n",
    "    roc = roc.sort_values(by='fpr')\n",
    "    tpr = roc['tpr']\n",
    "    fpr = roc['fpr']\n",
    "    mean_fpr = np.linspace(0, 1, 21)\n",
    "    tpr = interp(mean_fpr, fpr, tpr)\n",
    "    return mean_fpr, tpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Filtering (just for this plot)\n",
    "# results_df = results_df[results_df['target'].isin(['HTN Heuristic', 'HTN Diagnosis'])]\n",
    "\n",
    "# Calculating rocauc and auprc\n",
    "spacing, fontsize = 3, 14\n",
    "for target, perf_t in results_df.groupby('target'):\n",
    "    # if target == 'Resistant HTN Diagnosis': continue\n",
    "    # if target == 'HTN-Hypokalemia Diagnosis': continue\n",
    "\n",
    "    target_new = dnames_to_ugly[target]\n",
    "    print(target, target_new, f\"(shape {perf_t.shape})\", sep=\",\")\n",
    "\n",
    "#     ax = fig.add_subplot(2,3, i) \n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    # axis for PR curve\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    # axis for ROC curve\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    \n",
    "    i = 1\n",
    "    for m, model_nice in enumerate([o for o in order if o in perf_t.model.unique()]):\n",
    "        # model = nice_to_ugly[model_nice]\n",
    "        perf_t_m = perf_t.loc[perf_t.model==model_nice] \n",
    "        \n",
    "        print(f'- graphing {model_nice} - shape {perf_t_m.shape}')\n",
    "\n",
    "        mean_run_precisions = []\n",
    "        mean_run_tprs = []\n",
    "        if i == 1 and target_new in heuristics.keys():\n",
    "            mean_run_precision_h = []\n",
    "            mean_run_recall_h = []\n",
    "            mean_run_fpr_h = []\n",
    "            mean_run_tpr_h = []\n",
    "\n",
    "        for RunID, perf_t_m_id in perf_t_m.groupby('RunID'):\n",
    "            precisions = []\n",
    "            tprs = []\n",
    "            precisions_h = []\n",
    "            recalls_h = []\n",
    "            fprs_h = []\n",
    "            tprs_h = []\n",
    "\n",
    "            for fold, perf_t_m_id_f in perf_t_m_id.groupby('fold'):\n",
    "\n",
    "                #True labels\n",
    "                df = pd.read_csv('../data/Dataset' + str(RunID) + '/' +\n",
    "                                 target_new + '/' + target_new + fold +\n",
    "                                 'Test.csv')\n",
    "                                 \n",
    "                y = df[targets_rev[target_new]].values\n",
    "                \n",
    "                for random_state, perf_t_m_id_f_r in perf_t_m_id_f.groupby('random_state'):\n",
    "                    print(\" -\", RunID, fold, random_state)\n",
    "\n",
    "                    # handle the heuristic\n",
    "                    if i == 1 and target_new in heuristics.keys():\n",
    "                        y_heuristic = df[heuristics[target_new]].values\n",
    "\n",
    "                        # print('y_heuristic:', y_heuristic)\n",
    "\n",
    "                        precision_h = np.sum((y==1) & (y_heuristic==1))/np.sum(y_heuristic==1)\n",
    "                        recall_h = np.sum((y==1) & (y_heuristic==1))/np.sum(y==1)\n",
    "\n",
    "                        # print('precision_h:',precision_h)\n",
    "                        # print('recall_h:',recall_h)\n",
    "                        \n",
    "                        precisions_h.append(precision_h)\n",
    "                        recalls_h.append(recall_h)\n",
    "\n",
    "                        fpr_h = np.sum((y==0) & (y_heuristic==1))/np.sum(y==0) \n",
    "                        tpr_h = recall_h\n",
    "\n",
    "                        # print('fpr_h:',fpr_h)\n",
    "                        # print('tpr_h:',tpr_h)\n",
    "                    \n",
    "                        fprs_h.append(fpr_h)\n",
    "                        tprs_h.append(tpr_h)\n",
    "\n",
    "                        heuristic=False\n",
    "\n",
    "                    # print('y:',len(y))\n",
    "\n",
    "                    #Predicted probabilities\n",
    "                    assert(len(perf_t_m_id_f_r)==1)\n",
    "\n",
    "                    # print(perf_t_m_id_f_r['pred_proba'].values[0])\n",
    "                    # print(type(perf_t_m_id_f_r['pred_proba'].values[0]))\n",
    "                    # y_pred_proba = eval(perf_t_m_id_f_r['pred_proba'].values[0])\n",
    "                    y_pred_proba = perf_t_m_id_f_r['pred_proba'].values[0]\n",
    "                \n",
    "                    # Precision / Recall\n",
    "                    ####################\n",
    "                    mean_recall, precision = prc_values(y,y_pred_proba)\n",
    "                    precisions.append(precision)\n",
    "                \n",
    "                    # ROC\n",
    "                    #####\n",
    "                    mean_fpr, tpr = roc_values(y,y_pred_proba)\n",
    "                    tprs.append(tpr)\n",
    "                \n",
    "                #mean_run_precisions: The mean of five fold precisions\n",
    "                mean_run_precisions.append(np.mean(precisions, axis=0))\n",
    "                #mean_run_tprs: The mean of five fold tprs\n",
    "                mean_run_tprs.append(np.mean(tprs, axis=0))\n",
    "                if i == 1 and target_new in heuristics.keys():\n",
    "                    mean_run_precision_h.append(np.mean(precisions_h, axis=0))\n",
    "                    mean_run_recall_h.append(np.mean(recalls_h, axis=0))\n",
    "                    mean_run_fpr_h.append(np.mean(fprs_h, axis=0))\n",
    "                    mean_run_tpr_h.append(np.mean(tprs_h, axis=0))\n",
    "\n",
    "        #mean_precisions: The mean of mean_run_precisions over 50 iterations\n",
    "        mean_precisions = np.mean(mean_run_precisions, axis=0)\n",
    "        #mean_tprs: The mean of mean_run_tprs over 50 iterations\n",
    "        mean_tprs = np.mean(mean_run_tprs, axis=0)\n",
    "        \n",
    "#         plt.figure(target_new, figsize=(10, 6))\n",
    "        # Precision/Recall plot \n",
    "        ax1.plot(mean_recall, mean_precisions, \n",
    "                 alpha=1,\n",
    "                 label=model_nice,\n",
    "                 marker = marker_choice[model_nice], \n",
    "                 markevery=spacing)\n",
    "        # ROC plot\n",
    "        #####\n",
    "        ax2.plot(mean_fpr, mean_tprs, \n",
    "                 alpha=1,\n",
    "                 label=model_nice,\n",
    "#                  marker = markers[m], \n",
    "                 marker = marker_choice[model_nice], \n",
    "                 markevery=spacing)\n",
    "        ax2.plot([0,1],[0,1],'--k',label=None)\n",
    "        i+=1\n",
    "            \n",
    "    # heuristic performance\n",
    "#     print('mean_run_precision_h:',mean_run_precision_h)\n",
    "#     print('mean_run_recall_h:',mean_run_recall_h)\n",
    "#     print('mean_run_fpr_h:',mean_run_fpr_h)\n",
    "#     print('mean_run_tpr_h:',mean_run_tpr_h)\n",
    "    mean_recall_h = np.mean(mean_run_recall_h, axis=0)\n",
    "    mean_precision_h = np.mean(mean_run_precision_h, axis=0)\n",
    "    mean_fpr_h = np.mean(mean_run_fpr_h, axis=0)\n",
    "    mean_tpr_h = np.mean(mean_run_tpr_h, axis=0)\n",
    "    print(mean_recall_h, mean_precision_h, mean_fpr_h, mean_tpr_h)\n",
    "    # plot heuristic\n",
    "    ax1.plot(mean_recall_h,\n",
    "             mean_precision_h,\n",
    "             'Xk',\n",
    "             label='Heuristic',\n",
    "            ) \n",
    "    # plot heuristic\n",
    "    ax2.plot(mean_fpr_h,\n",
    "             mean_tpr_h,\n",
    "             'Xk',\n",
    "             label='Heuristic',\n",
    "            ) \n",
    "    plt.suptitle(dnames_to_nice[target_new], fontsize=fontsize)\n",
    "    ax1.set_xlabel(\"Recall (Sensitivity)\", fontsize=fontsize)\n",
    "    ax1.set_ylabel(\"Precision\", fontsize=fontsize)\n",
    "    ax2.set_xlabel(\"1 - Specificity\", fontsize=fontsize)\n",
    "    ax2.set_ylabel(\"Sensitivity\", fontsize=fontsize)\n",
    "#     if i in [1,4]:\n",
    "#         plt.xlabel(\"Recall (Sensitivity)\")\n",
    "#     if i > 3:\n",
    "#         plt.ylabel(\"Precision (PPV)\")\n",
    "#     else:\n",
    "#         plt.xticks([])\n",
    "#     if i == 6:        \n",
    "#         plt.legend()\n",
    "    \n",
    "#     i += 1\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    for filetype in ['.svg','.png','.pdf']:\n",
    "        plt.savefig(f'{paper_dir}/{target_new}_PRC_ROC{filetype}', dpi=400)\n",
    "\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Calculating rocauc and auprc\n",
    "spacing, fontsize = 3, 14\n",
    "for target, perf_t in results_df.groupby('target'):\n",
    "    target_new = dnames_to_ugly[target]\n",
    "    print(target, target_new, f\"(shape {perf_t.shape})\", sep=\",\")\n",
    "\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    \n",
    "    i = 1\n",
    "    for m, model_nice in enumerate([o for o in order if o in perf_t.model.unique()]):\n",
    "        perf_t_m = perf_t.loc[perf_t.model==model_nice] \n",
    "        \n",
    "        print(f'- graphing {model_nice} - shape {perf_t_m.shape}')\n",
    "\n",
    "        mean_run_precisions = []\n",
    "        mean_run_tprs = []\n",
    "        if i == 1 and target_new in heuristics.keys():\n",
    "            mean_run_precision_h = []\n",
    "            mean_run_recall_h = []\n",
    "            mean_run_fpr_h = []\n",
    "            mean_run_tpr_h = []\n",
    "\n",
    "        for RunID, perf_t_m_id in perf_t_m.groupby('RunID'):\n",
    "            precisions = []\n",
    "            tprs = []\n",
    "            precisions_h = []\n",
    "            recalls_h = []\n",
    "            fprs_h = []\n",
    "            tprs_h = []\n",
    "\n",
    "            for fold, perf_t_m_id_f in perf_t_m_id.groupby('fold'):\n",
    "\n",
    "                #True labels\n",
    "                df = pd.read_csv('../data/Dataset' + str(RunID) + '/' +\n",
    "                                 target_new + '/' + target_new + fold +\n",
    "                                 'Test.csv')\n",
    "                                 \n",
    "                y = df[targets_rev[target_new]].values\n",
    "                \n",
    "                for random_state, perf_t_m_id_f_r in perf_t_m_id_f.groupby('random_state'):\n",
    "                    print(\" -\", RunID, fold, random_state)\n",
    "\n",
    "                    # handle the heuristic\n",
    "                    if i == 1 and target_new in heuristics.keys():\n",
    "                        y_heuristic = df[heuristics[target_new]].values\n",
    "\n",
    "                        # print('y_heuristic:', y_heuristic)\n",
    "\n",
    "                        precision_h = np.sum((y==1) & (y_heuristic==1))/np.sum(y_heuristic==1)\n",
    "                        recall_h = np.sum((y==1) & (y_heuristic==1))/np.sum(y==1)\n",
    "\n",
    "                        # print('precision_h:',precision_h)\n",
    "                        # print('recall_h:',recall_h)\n",
    "                        \n",
    "                        precisions_h.append(precision_h)\n",
    "                        recalls_h.append(recall_h)\n",
    "\n",
    "                        fpr_h = np.sum((y==0) & (y_heuristic==1))/np.sum(y==0) \n",
    "                        tpr_h = recall_h\n",
    "\n",
    "                        # print('fpr_h:',fpr_h)\n",
    "                        # print('tpr_h:',tpr_h)\n",
    "                    \n",
    "                        fprs_h.append(fpr_h)\n",
    "                        tprs_h.append(tpr_h)\n",
    "\n",
    "                        heuristic=False\n",
    "\n",
    "                    # print('y:',len(y))\n",
    "\n",
    "                    #Predicted probabilities\n",
    "                    assert(len(perf_t_m_id_f_r)==1)\n",
    "\n",
    "                    # print(perf_t_m_id_f_r['pred_proba'].values[0])\n",
    "                    # print(type(perf_t_m_id_f_r['pred_proba'].values[0]))\n",
    "                    # y_pred_proba = eval(perf_t_m_id_f_r['pred_proba'].values[0])\n",
    "                    y_pred_proba = perf_t_m_id_f_r['pred_proba'].values[0]\n",
    "                \n",
    "                    # Precision / Recall\n",
    "                    ####################\n",
    "                    mean_recall, precision = prc_values(y,y_pred_proba)\n",
    "                    precisions.append(precision)\n",
    "                \n",
    "                    # ROC\n",
    "                    #####\n",
    "                    mean_fpr, tpr = roc_values(y,y_pred_proba)\n",
    "                    tprs.append(tpr)\n",
    "                \n",
    "                mean_run_precisions.append(np.mean(precisions, axis=0))\n",
    "                mean_run_tprs.append(np.mean(tprs, axis=0))\n",
    "\n",
    "                if i == 1 and target_new in heuristics.keys():\n",
    "                    mean_run_precision_h.append(np.mean(precisions_h, axis=0))\n",
    "                    mean_run_recall_h.append(np.mean(recalls_h, axis=0))\n",
    "                    mean_run_fpr_h.append(np.mean(fprs_h, axis=0))\n",
    "                    mean_run_tpr_h.append(np.mean(tprs_h, axis=0))\n",
    "\n",
    "        mean_run_precisions = np.array(mean_run_precisions)\n",
    "        mean_run_tprs = np.array(mean_run_tprs)\n",
    "\n",
    "        mean_precisions = np.mean(mean_run_precisions, axis=0)\n",
    "        mean_tprs = np.mean(mean_run_tprs, axis=0)\n",
    "\n",
    "        # Precision/Recall plot \n",
    "        best_run = np.argmax(np.sum(mean_run_precisions, axis=1))\n",
    "        ax1.plot(mean_recall, mean_run_precisions[best_run], \n",
    "                 alpha=1,\n",
    "                 label=model_nice,\n",
    "                 marker = marker_choice[model_nice], \n",
    "                 markevery=spacing)\n",
    "        \n",
    "        # ROC plot\n",
    "        best_run = np.argmax(np.sum(mean_run_tprs, axis=1))\n",
    "        ax2.plot(mean_fpr, mean_run_tprs[best_run], \n",
    "                 alpha=1,\n",
    "                 label=model_nice,\n",
    "                 marker = marker_choice[model_nice], \n",
    "                 markevery=spacing)\n",
    "        \n",
    "        ax2.plot([0,1],[0,1],'--k',label=None)\n",
    "\n",
    "        i+=1\n",
    "            \n",
    "    mean_recall_h = np.mean(mean_run_recall_h, axis=0)\n",
    "    mean_precision_h = np.mean(mean_run_precision_h, axis=0)\n",
    "    mean_fpr_h = np.mean(mean_run_fpr_h, axis=0)\n",
    "    mean_tpr_h = np.mean(mean_run_tpr_h, axis=0)\n",
    "\n",
    "    print(mean_recall_h, mean_precision_h, mean_fpr_h, mean_tpr_h)\n",
    "\n",
    "    # plot heuristic\n",
    "    ax1.plot(mean_recall_h,\n",
    "             mean_precision_h,\n",
    "             'Xk',\n",
    "             label='Heuristic',\n",
    "            ) \n",
    "    # plot heuristic\n",
    "    ax2.plot(mean_fpr_h,\n",
    "             mean_tpr_h,\n",
    "             'Xk',\n",
    "             label='Heuristic',\n",
    "            ) \n",
    "    \n",
    "    plt.suptitle(dnames_to_nice[target_new], fontsize=fontsize)\n",
    "    ax1.set_xlabel(\"Recall (Sensitivity)\", fontsize=fontsize)\n",
    "    ax1.set_ylabel(\"Precision\", fontsize=fontsize)\n",
    "    ax2.set_xlabel(\"1 - Specificity\", fontsize=fontsize)\n",
    "    ax2.set_ylabel(\"Sensitivity\", fontsize=fontsize)\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    for filetype in ['.svg','.png','.pdf']:\n",
    "        plt.savefig(f'{paper_dir}/{target_new}_PRC_ROC_BEST_MODEL{filetype}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copying Fig.2 from paper \"A flexible symbolic regression method for constructing\n",
    "# # interpretable clinical prediction models\"\n",
    "\n",
    "# for stat_comparisons in [False, True]:\n",
    "#     for yaxis in [\n",
    "#         'average_precision_score_test', \n",
    "#         'roc_auc_score_test', \n",
    "#         'accuracy_score_test', \n",
    "#         'size'\n",
    "#     ]:\n",
    "#         data = results_df_melted[results_df_melted['variable']==yaxis].dropna()\n",
    "                    \n",
    "#         yaxis = yaxis.replace('_', ' ').capitalize()\n",
    "\n",
    "#         data = data.rename(columns={'value': yaxis, 'model':'Model'})\n",
    "#         data[yaxis] = data[yaxis].astype(float)\n",
    "\n",
    "#         print(data.random_state.unique())\n",
    "#         display(data)\n",
    "\n",
    "#         g = sns.catplot(\n",
    "#             data=data,\n",
    "#             x=\"Model\", y=yaxis, col=\"target\",\n",
    "#             order=order, col_wrap=3, col_order=phenotypes_order,\n",
    "#             aspect= 1 if stat_comparisons else 0.825,\n",
    "#             height= 5 if stat_comparisons else 3.25,\n",
    "#             estimator=np.mean, sharey=not stat_comparisons,\n",
    "#             kind=\"box\", **boxplot_kwargs\n",
    "#         )\n",
    "#         g.refline(x=5.5, color='k', lw=0.5, ls='--', zorder=0)\n",
    "        \n",
    "#         for (title, xaxis) in g._axes_dict.items():\n",
    "#             xaxis.set_title(title)\n",
    "#             xaxis.grid(which='major', axis='y', linewidth=.8)\n",
    "#             xaxis.grid(which='major', axis='x', linewidth=.5)\n",
    "            \n",
    "#             for tick in xaxis.get_xticklabels():            \n",
    "#                 if stat_comparisons:\n",
    "#                     tick.set(rotation=45, ha='right', va='top', ma='center')\n",
    "#                 else:\n",
    "#                     tick.set(rotation=90, ha='right', va='top', ma='center')\n",
    "\n",
    "#             if yaxis=='Size':\n",
    "#                 xaxis.set_yscale('log')\n",
    "\n",
    "#             # All vs baseline\n",
    "#             llms = [\n",
    "#                 'gpt-4o-mini',\n",
    "#                 'gpt-4o-mini-iter',\n",
    "#                 'gpt-4o',\n",
    "#                 'gpt-4o-iter',\n",
    "#                 'gpt-4-turbo',\n",
    "#                 'gpt-4-turbo-iter',\n",
    "#             ]\n",
    "#             pairs = (\n",
    "#                 # list(itertools.product(llms, llms)) +\n",
    "#                 [(llm, 'FEAT') for llm in llms ]\n",
    "#             )\n",
    "\n",
    "#             if stat_comparisons:\n",
    "#                 try:\n",
    "#                     annotator = Annotator(\n",
    "#                         xaxis, pairs, data=data[(data['target']==title)][\n",
    "#                             ['Model', 'target',\tyaxis]\n",
    "#                         ],\n",
    "#                         x='Model', y=yaxis,\n",
    "#                         order=order)\n",
    "#                     annotator.reset_configuration()\n",
    "#                     annotator.configure(test=\"Mann-Whitney\", hide_non_significant=True, text_format='simple',\n",
    "#                                         comparisons_correction=\"holm\", verbose=0) # , text_offset=-3\n",
    "                    \n",
    "#                     # Calculate and annotate\n",
    "#                     annotator.apply_and_annotate()\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"failed to add statsannotation with dataset {title}. Except {e}\")\n",
    "\n",
    "#         g.map_dataframe(sns.stripplot, y=yaxis, dodge=True, \n",
    "#                     x=\"Model\", order=order, size=2, color='k',\n",
    "#                     linewidth=0.5, alpha=0.25)\n",
    "    \n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f\"{paper_dir}/fig2_{yaxis}{'_stat_comparisons' if stat_comparisons else ''}.pdf\")\n",
    "#         plt.savefig(f\"{paper_dir}/fig2_{yaxis}{'_stat_comparisons' if stat_comparisons else ''}.png\")\n",
    "#         plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
